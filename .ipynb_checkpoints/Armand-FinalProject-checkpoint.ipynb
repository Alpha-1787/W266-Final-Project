{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to pull data from Reddit\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api\n",
    "\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import multiprocessing\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .sql that pulls comments\n",
    "fd = open('comments.sql', 'r')\n",
    "comment_sql = fd.read()\n",
    "fd.close()\n",
    "\n",
    "# Put query results into df\n",
    "comment_df = pd.read_gbq(comment_sql,\n",
    "                         project_id='w266-240122',\n",
    "                         dialect='standard')\n",
    "\n",
    "# Convert date into proper date/time\n",
    "comment_df['created_dt_tm'] = comment_df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Create field for month\n",
    "comment_df['created_dt_month'] = comment_df['created_dt_tm'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3049: DtypeWarning: Columns (0,1,2,4,5,6,9,10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "#comment_df = pd.read_csv('reddit_comments_2016_2019.csv', parse_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read .sql that pulls comments\n",
    "fd = open('posts.sql', 'r')\n",
    "post_sql = fd.read()\n",
    "fd.close()\n",
    "\n",
    "post_df = pd.read_gbq(post_sql,\n",
    "                      project_id='w266-240122',\n",
    "                      dialect='standard')\n",
    "\n",
    "# Convert date into proper date/time\n",
    "post_df['created_dt_tm'] = post_df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Create field for month\n",
    "post_df['created_dt_month'] = post_df['created_dt_tm'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#post_df = pd.read_csv('reddit_posts_2016_2019.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    302238.000000\n",
       "mean         46.788485\n",
       "std         259.006361\n",
       "min           0.000000\n",
       "25%           1.000000\n",
       "50%           6.000000\n",
       "75%          23.000000\n",
       "max       54062.000000\n",
       "Name: score, dtype: float64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df['score'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(comment_df, post_df, subreddit, lower=False):\n",
    "    \n",
    "    # TODO remove symbols, contractions\n",
    "    # TODO lowercase words?\n",
    "    # TODO Treat numbers as something else\n",
    "    # Add beginning and end of sentence?\n",
    "    # TODO how do we deal with stop words?\n",
    "    # UNK token\n",
    "    \n",
    "    # Body text from post dataframe\n",
    "    post_text = list(post_df[(post_df['is_self'] == True) &\n",
    "                             (~post_df['selftext'].isin(['[removed]', '[deleted]'])) &\n",
    "                             post_df['subreddit'].isin(subreddit)]['selftext'].unique())\n",
    "    \n",
    "    # Add in text from title\n",
    "    post_text += list(post_df[post_df['is_self'] == True]['title'].unique())\n",
    "    \n",
    "    # Add in text from post comments\n",
    "    post_text += list(comment_df[(comment_df['subreddit'].isin(subreddit)) & \n",
    "                                     (~comment_df['body'].isin(['[removed]', '[deleted]']))]['body'])\n",
    "    \n",
    "    # Put all text into dataframe and drop dupes\n",
    "    text_df = pd.DataFrame(post_text, columns=['text'])\n",
    "    text_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Tokenize at sentence level\n",
    "    text_df['sent_tokenized'] = text_df['text'].apply(nltk.sent_tokenize)\n",
    "    text_df['sent_count'] = text_df['sent_tokenized'].apply(lambda x: len(x))\n",
    "\n",
    "    # Put tokenized body into a list\n",
    "    sent_list = list(text_df[text_df['sent_count'] > 0]['sent_tokenized'].apply(pd.Series).stack().unique())\n",
    "\n",
    "    # Put list into dataframe\n",
    "    sent_df = pd.DataFrame(sent_list, columns=['sentence'])\n",
    "    \n",
    "    # lowercase\n",
    "    if lower == True:\n",
    "        sent_df['sentence'] = sent_df['sentence'].str.lower()\n",
    "\n",
    "    # Tokenize each sentence at the word level\n",
    "    sent_df['word_token'] = sent_df['sentence'].apply(nltk.word_tokenize)\n",
    "    \n",
    "    return sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(sent_df):\n",
    "    \n",
    "    # train word embedding\n",
    "    embedding = Word2Vec(list(sent_df['word_token']),\n",
    "                         size=100,\n",
    "                         window=5, \n",
    "                         min_count=5, \n",
    "                         negative=15, \n",
    "                         iter=10, workers=6)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-03-01    109492\n",
       "2018-10-01    107297\n",
       "2019-01-01    100792\n",
       "2018-11-01     91601\n",
       "2019-02-01     88902\n",
       "2018-12-01     73725\n",
       "2018-09-01       765\n",
       "Name: created_dt_month, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df['created_dt_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-10-01    11185\n",
       "2019-01-01    10551\n",
       "2019-03-01    10473\n",
       "2018-11-01     9703\n",
       "2019-02-01     9316\n",
       "2018-12-01     7986\n",
       "2018-09-01       94\n",
       "Name: created_dt_month, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df['created_dt_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = preprocess_text(comment_df, \n",
    "                          post_df,\n",
    "                          ['democrats', 'progressive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['sent_length'] = sent_df['word_token'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df['score_sentiment'] = comment_df['score'].apply(lambda x: 'positive' if x > 0 \n",
    "                                                          else 'negative' if x < 0 \n",
    "                                                          else 'neutral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    2306098\n",
       "negative     207636\n",
       "neutral      163302\n",
       "Name: score_sentiment, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df.score_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"And that still isn't disgusting or cringe inducing? Self flagellating Whites make me sick. No other race on the planet does this.\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df.iloc[0]['body']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df = comment_df[['subreddit', 'body', 'created_dt_month', 'score', 'score_sentiment']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_df['word_tokens'] = comment_df['body'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emebddings from Democratic Leaning Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store the embeddings\n",
    "embedding_list = []\n",
    "\n",
    "# Dates to iterate over\n",
    "dates = comment_df.created_dt_month.sort_values().unique()[1:]\n",
    "\n",
    "# Iterate for each month and create embedding for it\n",
    "for d in dates:\n",
    "    sent_df = preprocess_text(comment_df[comment_df['created_dt_month'] == d], \n",
    "                              post_df[post_df['created_dt_month'] == d],\n",
    "                              ['democrats', 'progressive'], \n",
    "                              lower=False)\n",
    "    word_embedding = train_embedding(sent_df)\n",
    "    embedding_list.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = preprocess_text(comment_df, \n",
    "                          post_df,\n",
    "                          ['democrats', 'progressive'], \n",
    "                          lower=False)\n",
    "word_embedding = train_embedding(sent_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lexicon(filename):\n",
    "    \"\"\"\n",
    "    Load a file from Bing Liu's sentiment lexicon\n",
    "    (https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html), containing\n",
    "    English words in Latin-1 encoding.\n",
    "    \n",
    "    One file contains a list of positive words, and the other contains\n",
    "    a list of negative words. The files contain comment lines starting\n",
    "    with ';' and blank lines, which should be skipped.\n",
    "    \"\"\"\n",
    "    lexicon = []\n",
    "    with open(filename, encoding='latin-1') as infile:\n",
    "        for line in infile:\n",
    "            line = line.rstrip()\n",
    "            if line and not line.startswith(';'):\n",
    "                lexicon.append(line)\n",
    "    return lexicon\n",
    "\n",
    "pos_words = load_lexicon('positive-words.txt')\n",
    "neg_words = load_lexicon('negative-words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2006"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_embedding' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-60297874b00f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0membedding_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0membedding_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mpos_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'word_embedding' is not defined"
     ]
    }
   ],
   "source": [
    "e = word_embedding\n",
    "embedding_df = pd.DataFrame(e.wv.vectors)\n",
    "embedding_df.index = e.wv.index2word\n",
    "\n",
    "pos_vectors = embedding_df.reindex(index=pos_words).dropna()\n",
    "neg_vectors = embedding_df.reindex(index=neg_words).dropna()\n",
    "\n",
    "vectors = pd.concat([pos_vectors, neg_vectors])\n",
    "targets = np.array([1 for entry in pos_vectors.index] + [-1 for entry in neg_vectors.index])\n",
    "labels = list(pos_vectors.index) + list(neg_vectors.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7634854771784232\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:152: DeprecationWarning: n_iter parameter is deprecated in 0.19 and will be removed in 0.21. Use max_iter and tol instead.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "train_vectors, test_vectors, train_targets, test_targets, train_labels, test_labels = \\\n",
    "train_test_split(vectors, targets, labels, test_size=0.1, random_state=0)\n",
    "model = SGDClassifier(loss='log', random_state=0, n_iter=100)\n",
    "model.fit(train_vectors, train_targets)\n",
    "\n",
    "print(accuracy_score(model.predict(test_vectors), test_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sabotage</th>\n",
       "      <td>-2.115922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undocumented</th>\n",
       "      <td>-2.074377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slowly</th>\n",
       "      <td>-0.855885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>slaughter</th>\n",
       "      <td>-0.999751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>debilitating</th>\n",
       "      <td>0.157101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dirt</th>\n",
       "      <td>-2.300752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mindless</th>\n",
       "      <td>-1.347088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>strong</th>\n",
       "      <td>13.202846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nuisance</th>\n",
       "      <td>-0.612164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deceiving</th>\n",
       "      <td>-0.432331</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              sentiment\n",
       "sabotage      -2.115922\n",
       "undocumented  -2.074377\n",
       "slowly        -0.855885\n",
       "slaughter     -0.999751\n",
       "debilitating   0.157101\n",
       "dirt          -2.300752\n",
       "mindless      -1.347088\n",
       "strong        13.202846\n",
       "nuisance      -0.612164\n",
       "deceiving     -0.432331"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vecs_to_sentiment(vecs):\n",
    "    # predict_log_proba gives the log probability for each class\n",
    "    predictions = model.predict_log_proba(vecs)\n",
    "\n",
    "    # To see an overall positive vs. negative classification in one number,\n",
    "    # we take the log probability of positive sentiment minus the log\n",
    "    # probability of negative sentiment.\n",
    "    return predictions[:, 1] - predictions[:, 0]\n",
    "\n",
    "\n",
    "def words_to_sentiment(words):\n",
    "    vecs = embedding_df.loc[words].dropna()\n",
    "    log_odds = vecs_to_sentiment(vecs)\n",
    "    return pd.DataFrame({'sentiment': log_odds}, index=vecs.index)\n",
    "\n",
    "\n",
    "# Show 20 examples from the test set\n",
    "words_to_sentiment(test_labels).iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "TOKEN_RE = re.compile(r\"\\w.*?\\b\")\n",
    "# The regex above finds tokens that start with a word-like character (\\w), and continues\n",
    "# matching characters (.+?) until the next word break (\\b). It's a relatively simple\n",
    "# expression that manages to extract something very much like words from text.\n",
    "\n",
    "\n",
    "def text_to_sentiment(text):\n",
    "    tokens = [token.casefold() for token in TOKEN_RE.findall(text)]\n",
    "    sentiments = words_to_sentiment(tokens)\n",
    "    return sentiments['sentiment'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.745474703785556"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.1328254916227443"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment('Obama')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2195910751765513"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment('Biden')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.283264240040666"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment('Progressive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08479791049252694"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_sentiment('Mexican')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 0.8514634370803833),\n",
       " ('Putin', 0.6282497644424438),\n",
       " ('he', 0.5907704830169678),\n",
       " ('Trumpski', 0.590567946434021),\n",
       " ('tRump', 0.579472541809082),\n",
       " ('him', 0.5785919427871704),\n",
       " ('Russia', 0.5719641447067261),\n",
       " ('Bernie', 0.5676500797271729),\n",
       " ('Whitaker', 0.5499702095985413),\n",
       " ('Obama', 0.5409168004989624)]"
      ]
     },
     "execution_count": 371,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.wv.most_similar('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tRump', 0.6996694207191467),\n",
       " ('wrist', 0.6700129508972168),\n",
       " ('stuffed', 0.6529507040977478),\n",
       " ('Bitch', 0.6497117280960083),\n",
       " ('trumpty', 0.6405702829360962),\n",
       " ('Pai', 0.6387597322463989),\n",
       " ('graham', 0.6387059688568115),\n",
       " ('Ajit', 0.638285219669342),\n",
       " ('tRUMP', 0.6369954347610474),\n",
       " ('Haley', 0.6360580325126648)]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.wv.most_similar('Donny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('impeaching', 0.6418150067329407),\n",
       " ('Impeach', 0.6205568909645081),\n",
       " ('Supporters', 0.6122241020202637),\n",
       " ('Supporter', 0.6121823787689209),\n",
       " ('Tower', 0.5998998880386353),\n",
       " ('Organization', 0.5910106897354126),\n",
       " ('Pence', 0.5623512864112854),\n",
       " ('President', 0.5594371557235718),\n",
       " ('Fred', 0.5535253882408142),\n",
       " ('administration', 0.5455576777458191)]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.wv.most_similar('Donald')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  331M    0   415    0     0    369      0  10d 21h  0:00:01  10d 21h   369x assets/\n",
      "x saved_model.pb\n",
      "x tfhub_module.pb\n",
      "x variables/\n",
      "x variables/variables.index\n",
      "100  331M  100  331M    0     0  2054k      0  0:02:45  0:02:45 --:--:-- 2379k 2614kk      0  0:02:59  0:01:26  0:01:33 2755k   0  2087k      0  0:02:42  0:01:43  0:00:59 3369k      0  0:02:41  0:01:51  0:00:50 2300k  0:01:56  0:00:44 2391k0:02:44  0:02:25  0:00:19 2082k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download ELMo to a local directory\n",
    "!mkdir module_elmo\n",
    "\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "!curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC module_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'hub' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-511b60129c9d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Initialize elmo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0melmo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'module_elmo'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melmo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Trump'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"default\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elmo'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Get word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'hub' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize elmo\n",
    "elmo = hub.Module('module_elmo', trainable=True)\n",
    "embeddings = elmo(['Trump'], signature=\"default\", as_dict=True)['elmo']\n",
    "\n",
    "# Get word embedding\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download ELMo to a local directory\n",
    "!mkdir module_elmo\n",
    "\n",
    "# Download the module, and uncom`press it to the destination folder. \n",
    "!curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC module_elmo\n",
    "\n",
    "# Initialize elmo\n",
    "elmo = hub.Module('module_elmo', trainable=True)\n",
    "embeddings = elmo(['This is an example sentence that I want an embedding for'], signature=\"default\", as_dict=True)['elmo']\n",
    "\n",
    "# Get word embedding\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embeddings)\n",
    "\n",
    "message_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* Sentiment analysis labeled data\n",
    "* How would we go about scoring the performance of our models?\n",
    "    * I proposed to use the scores that are available on the reddit\n",
    "* Should we train the sentiment classifier at a sentence or post level?\n",
    "    * Each post may contain multiple sentences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
