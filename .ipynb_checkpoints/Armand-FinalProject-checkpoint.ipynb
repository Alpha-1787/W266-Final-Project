{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used to pull data from Reddit\n",
    "import praw\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import multiprocessing\n",
    "from google.cloud import bigquery\n",
    "\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = '''\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2018_10` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2018_11` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2018_12` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2019_01` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2019_02` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.body\n",
    "FROM \n",
    "  `fh-bigquery.reddit_comments.2019_03` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "'''\n",
    "\n",
    "# Put query results into df\n",
    "comment_df = pd.read_gbq(sql_query,\n",
    "                         project_id='w266-240122',\n",
    "                         dialect='standard')\n",
    "\n",
    "# Convert date into proper date/time\n",
    "comment_df['created_dt_tm'] = comment_df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Create field for month\n",
    "comment_df['created_dt_month'] = comment_df['created_dt_tm'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2018_10` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2018_11` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2018_12` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2019_01` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2019_02` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "UNION ALL\n",
    "SELECT \n",
    "    r.subreddit,\n",
    "    r.created_utc,\n",
    "    r.title,\n",
    "    r.selftext,\n",
    "    r.is_self\n",
    "FROM \n",
    "  `fh-bigquery.reddit_posts.2019_03` r\n",
    "WHERE\n",
    "  r.subreddit IN ('democrats', 'progressive', 'Conservative', 'republicans')\n",
    "\"\"\"\n",
    "\n",
    "post_df = pd.read_gbq(sql_query,\n",
    "                      project_id='w266-240122',\n",
    "                      dialect='standard')\n",
    "\n",
    "# Convert date into proper date/time\n",
    "post_df['created_dt_tm'] = post_df['created_utc'].apply(lambda x: dt.datetime.fromtimestamp(x))\n",
    "\n",
    "# Create field for month\n",
    "post_df['created_dt_month'] = post_df['created_dt_tm'].dt.to_period('M').dt.to_timestamp()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(comment_df, post_df, subreddit):\n",
    "    \n",
    "    # TODO remove symbols, contractions\n",
    "    # TODO lowercase words?\n",
    "    # TODO Treat numbers as something else\n",
    "    # Add beginning and end of sentence?\n",
    "    # TODO how do we deal with stop words?\n",
    "    \n",
    "    # Body text from post dataframe\n",
    "    post_text = list(post_df[(post_df['is_self'] == True) &\n",
    "                             (~post_df['selftext'].isin(['[removed]', '[deleted]'])) &\n",
    "                             post_df['subreddit'].isin(subreddit)]['selftext'].unique())\n",
    "    \n",
    "    # Add in text from title\n",
    "    post_text += list(post_df[post_df['is_self'] == True]['title'].unique())\n",
    "    \n",
    "    # Add in text from post comments\n",
    "    post_text += list(comment_df[(comment_df['subreddit'].isin(subreddit)) & \n",
    "                                     (~comment_df['body'].isin(['[removed]', '[deleted]']))]['body'])\n",
    "    \n",
    "    # Put all text into dataframe and drop dupes\n",
    "    text_df = pd.DataFrame(post_text, columns=['text'])\n",
    "    text_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # Tokenize at sentence level\n",
    "    text_df['sent_tokenized'] = text_df['text'].apply(nltk.sent_tokenize)\n",
    "    text_df['sent_count'] = text_df['sent_tokenized'].apply(lambda x: len(x))\n",
    "\n",
    "    # Put tokenized body into a list\n",
    "    sent_list = list(text_df[text_df['sent_count'] > 0]['sent_tokenized'].apply(pd.Series).stack().unique())\n",
    "\n",
    "    # Put list into dataframe\n",
    "    sent_df = pd.DataFrame(sent_list, columns=['sentence'])\n",
    "\n",
    "    # Tokenize each sentence at the word level\n",
    "    sent_df['word_token'] = sent_df['sentence'].apply(nltk.word_tokenize)\n",
    "    \n",
    "    return sent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_embedding(sent_df):\n",
    "    \n",
    "    # train word embedding\n",
    "    embedding = Word2Vec(list(sent_df['word_token']),\n",
    "                         size=100,\n",
    "                         window=5, \n",
    "                         min_count=5, \n",
    "                         negative=15, \n",
    "                         iter=10, workers=6)\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019-03-01    109492\n",
       "2018-10-01    107297\n",
       "2019-01-01    100792\n",
       "2018-11-01     91601\n",
       "2019-02-01     88902\n",
       "2018-12-01     73725\n",
       "2018-09-01       765\n",
       "Name: created_dt_month, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment_df['created_dt_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2018-10-01    11185\n",
       "2019-01-01    10551\n",
       "2019-03-01    10473\n",
       "2018-11-01     9703\n",
       "2019-02-01     9316\n",
       "2018-12-01     7986\n",
       "2018-09-01       94\n",
       "Name: created_dt_month, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df['created_dt_month'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df = preprocess_text(comment_df, \n",
    "                          post_df,\n",
    "                          ['democrats', 'progressive'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_df['sent_length'] = sent_df['word_token'].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    162849.000000\n",
       "mean         16.033215\n",
       "std          12.799296\n",
       "min           1.000000\n",
       "25%           8.000000\n",
       "50%          13.000000\n",
       "75%          21.000000\n",
       "max        1079.000000\n",
       "Name: sent_length, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_df['sent_length'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emebddings from Democratic Leaning Subreddits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store the embeddings\n",
    "embedding_list = []\n",
    "\n",
    "# Dates to iterate over\n",
    "dates = comment_df.created_dt_month.sort_values().unique()[1:]\n",
    "\n",
    "# Iterate for each month and create embedding for it\n",
    "for d in dates:\n",
    "    sent_df = preprocess_text(comment_df[comment_df['created_dt_month'] == d], \n",
    "                              post_df[post_df['created_dt_month'] == d],\n",
    "                             ['democrats', 'progressive'])\n",
    "    word_embedding = train_embedding(sent_df)\n",
    "    embedding_list.append(word_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.80972916,  0.492011  ,  0.6000299 ,  1.2146021 , -1.3632665 ,\n",
       "        2.1707451 ,  1.1129888 , -0.9950558 ,  1.3010796 ,  0.5712705 ,\n",
       "       -1.2671745 , -0.62562543, -0.92984444,  0.46017087,  1.2199383 ,\n",
       "       -0.4100108 ,  0.03662169,  0.08015236,  1.2353998 , -1.3475568 ,\n",
       "       -0.58328617,  0.81273687, -0.15185723, -2.462653  , -0.97824043,\n",
       "       -0.13730292, -0.7565513 , -0.5166247 ,  0.15975673, -0.42482996,\n",
       "        1.8343176 ,  1.0839666 ,  2.9047277 ,  0.46503282,  0.9390749 ,\n",
       "        1.7883556 , -0.9226195 , -0.20035595, -2.4565654 ,  0.39234143,\n",
       "       -0.31991667, -0.25074252,  1.4127734 ,  0.45056665, -1.9515611 ,\n",
       "       -1.2206664 , -0.44553062,  0.88539475,  0.9610484 , -1.0692703 ,\n",
       "       -2.4858835 , -0.2809876 ,  1.6519452 ,  0.99012554,  0.73341364,\n",
       "        1.3321676 ,  0.82524407, -0.8716901 ,  1.6215423 ,  2.38216   ,\n",
       "       -1.0240828 ,  1.3062047 , -0.70725965, -0.13494553, -1.6523948 ,\n",
       "        0.24945359,  0.39787325,  2.1457255 ,  0.90786445,  1.1652907 ,\n",
       "        3.4471986 , -0.27954596, -0.4136184 ,  0.76583487, -0.21320908,\n",
       "        2.4644997 ,  0.3329601 ,  0.6077051 ,  0.7817685 ,  0.9173821 ,\n",
       "       -0.12220345, -2.3177354 , -2.3452532 ,  1.3767986 , -1.184349  ,\n",
       "       -2.1410515 ,  0.9768359 ,  0.40125316,  1.4178357 , -0.2866724 ,\n",
       "       -1.7885617 ,  0.05488241, -1.3081685 ,  0.21015553, -0.83356315,\n",
       "       -0.0502981 , -1.2692075 , -0.5067047 ,  0.6118074 ,  0.3635869 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_list[0].wv.get_vector('Trump')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.8915286 , -1.151935  ,  2.3414488 , -0.20818773, -1.167915  ,\n",
       "        2.5763302 ,  1.2839091 ,  0.61914647,  0.84623116,  2.054744  ,\n",
       "       -0.23066214, -0.28181052,  0.14015807,  0.256338  ,  1.2995111 ,\n",
       "       -0.25579634, -1.6174064 , -0.39227232,  1.4809597 , -1.490577  ,\n",
       "       -0.31913316, -0.7786051 , -0.03675904, -1.254495  ,  0.61446774,\n",
       "       -0.35716337,  0.53983533, -3.046102  , -1.8612096 ,  0.08314294,\n",
       "        0.22914301, -0.7540506 ,  0.9290284 , -0.8153843 , -0.54973507,\n",
       "        0.6833854 , -0.27577436, -0.31562427, -0.5111186 ,  0.85463065,\n",
       "        1.7767773 , -0.26418447,  1.1176025 , -0.6057725 , -2.1974056 ,\n",
       "        0.22298011, -1.8298978 ,  0.93621516,  0.2934621 , -0.6416577 ,\n",
       "       -0.7475256 ,  0.51469076,  1.7106967 ,  0.25979805, -0.25564542,\n",
       "       -0.07108784, -0.3038908 ,  0.51928097,  2.5243244 ,  0.82808506,\n",
       "       -1.4417411 ,  1.8168366 , -0.55191374, -0.02796305, -0.09545928,\n",
       "       -0.3312568 , -0.6972091 ,  2.545727  ,  0.3973557 , -0.03328303,\n",
       "        3.3810396 , -1.1519146 ,  0.6172695 ,  1.2713035 ,  0.1633722 ,\n",
       "        0.6934172 , -0.326066  ,  2.1386855 ,  1.0152237 ,  2.1135247 ,\n",
       "       -0.42875347, -1.8630214 , -1.8342599 , -0.14907692, -0.7310892 ,\n",
       "       -0.31121823,  1.6441182 ,  0.9809783 ,  1.3210995 , -1.8866085 ,\n",
       "       -0.40535203,  0.7066546 , -1.1451786 , -0.58124006, -1.5821704 ,\n",
       "        0.3923788 , -2.256383  , -0.5181735 ,  0.99032646, -1.1611289 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_list[1].wv.get_vector('Trump')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELMo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "  0  331M    0   415    0     0    369      0  10d 21h  0:00:01  10d 21h   369x assets/\n",
      "x saved_model.pb\n",
      "x tfhub_module.pb\n",
      "x variables/\n",
      "x variables/variables.index\n",
      "100  331M  100  331M    0     0  2054k      0  0:02:45  0:02:45 --:--:-- 2379k 2614kk      0  0:02:59  0:01:26  0:01:33 2755k   0  2087k      0  0:02:42  0:01:43  0:00:59 3369k      0  0:02:41  0:01:51  0:00:50 2300k  0:01:56  0:00:44 2391k0:02:44  0:02:25  0:00:19 2082k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Download ELMo to a local directory\n",
    "!mkdir module_elmo\n",
    "\n",
    "# Download the module, and uncompress it to the destination folder. \n",
    "!curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC module_elmo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0715 22:15:39.981253 4808177088 deprecation.py:323] From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0715 22:15:40.683024 4808177088 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# Initialize elmo\n",
    "elmo = hub.Module('module_elmo', trainable=True)\n",
    "embeddings = elmo(['Trump'], signature=\"default\", as_dict=True)['elmo']\n",
    "\n",
    "# Get word embedding\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "message_embeddings[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 11, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download ELMo to a local directory\n",
    "!mkdir module_elmo\n",
    "\n",
    "# Download the module, and uncom`press it to the destination folder. \n",
    "!curl -L \"https://tfhub.dev/google/elmo/2?tf-hub-format=compressed\" | tar -zxvC module_elmo\n",
    "\n",
    "# Initialize elmo\n",
    "elmo = hub.Module('module_elmo', trainable=True)\n",
    "embeddings = elmo(['This is an example sentence that I want an embedding for'], signature=\"default\", as_dict=True)['elmo']\n",
    "\n",
    "# Get word embedding\n",
    "with tf.Session() as session:\n",
    "    session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "    message_embeddings = session.run(embeddings)\n",
    "\n",
    "message_embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions\n",
    "* Sentiment analysis labeled data\n",
    "* How would we go about scoring the performance of our models?\n",
    "    * I proposed to use the scores that are available on the reddit\n",
    "* Should we train the sentiment classifier at a sentence or post level?\n",
    "    * Each post may contain multiple sentences\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
